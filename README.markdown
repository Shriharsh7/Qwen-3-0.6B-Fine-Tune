# Qwen3-0.6B Fine-Tuning for Creative Writing

This repository contains the notebook `fine_tuned_qwen_3_0_6b_creative_writing_clean.ipynb`, which documents the fine-tuning of the Qwen/Qwen3-0.6B model for creative writing tasks, specifically storytelling and dialogue generation. The fine-tuning was performed using the Gryphe/ChatGPT-4o-Writing-Prompts dataset, resulting in a lightweight model capable of generating coherent and creative stories.

## HuggingFace link of the finetuned model
https://huggingface.co/Shriharsh/qwen3-0.6b-creative-writing

## Project Overview

The goal of this project was to fine-tune a smaller large language model (383M parameters) to generate high-quality creative writing outputs, such as stories and dialogues, using a dataset of prompt-response pairs from ChatGPT-4o. The fine-tuned model is available on Hugging Face at Shriharsh/qwen3-0.6b-creative-writing.

## Dataset

- **Dataset Used**: Gryphe/ChatGPT-4o-Writing-Prompts
- **Training Examples**: 700
- **Validation Examples**: 150
- **Test Examples**: 150
- **Description**: High-quality prompt-response pairs focused on creative writing, sourced from ChatGPT-4o.

## Fine-Tuning Process

### Model

- **Base Model**: Qwen/Qwen3-0.6B (383M parameters)

### Training Setup

- **Quantization and Adapters**: Used QLoRA with 4-bit quantization and LoRA adapters (r=8, lora_alpha=32) to enable efficient fine-tuning on a T4 GPU in Google Colab.
- **Hyperparameters**:
  - Epochs: 3
  - Learning Rate: 2e-4
  - Batch Size: Effective batch size of 4 (per_device_train_batch_size=1, gradient_accumulation_steps=4)
  - Training Regime: fp16 mixed precision
- **Training Time**: \~23 minutes for 525 steps (3 epochs)

### Results

- **Loss**:
  - Test Loss: 3.0295
  - Validation Loss: 3.0295 at step 525 (with slight overfitting observed as validation loss increased from 3.0217 at step 350 to 3.0569 at step 500)
- **Output**: The model generated a coherent story titled "The Timeless Pile", demonstrating its ability to handle creative writing prompts, though with limitations in depth due to the model's size.

## Usage

To use the fine-tuned model, you can load it from Hugging Face and generate text as follows:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Shriharsh/qwen3-0.6b-creative-writing"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")

prompt = "Write a short story about a lost traveler."
messages = [{"role": "user", "content": prompt}]
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = tokenizer(text, return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=2000, temperature=0.7, top_p=0.9)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

- **Note**: The model may output its "thinking" process before the final story. For a cleaner chatbot experience, consider post-processing the output to remove this section (e.g., using regex to strip text between `<think>` and `</think>`).

## Challenges and Solutions

- **Generation Error**: A `ValueError` occurred during generation when the input exceeded the `max_length`. This was addressed by setting `max_new_tokens=2000` to allow for longer outputs.

## Sample Output

Below is a sample story generated by the model in response to the prompt:

> "Write a short story about a lost time traveler who has a big pile of cash from the future which is unusable in the current time where he is now."

## Response

> **The Timeless Pile**
>
> Clara, a young woman from the future, found herself stranded in a small town in the present day after a malfunctioning time machine left her with a pile of future currencyâ€”worthless in her current time. Desperate to help her struggling family, she tried to use the money but faced confusion and rejection. As she grappled with the futility of her wealth, she realized the true cost of time and the value of the present moment.

## Future Improvements

- **Larger Dataset**: Fine-tuning on a larger dataset could reduce overfitting and improve generalization.
- **Hyperparameter Tuning**: Reducing the number of epochs (e.g., to 2) or adjusting the learning rate (e.g., to 1e-4) may help mitigate overfitting.
- **Model Size**: Fine-tuning a larger model (e.g., Qwen3-4B) could yield richer, more detailed outputs but would require more compute resources.

## License

This project is licensed under the Apache 2.0 license, inherited from the base Qwen3-0.6B model.

## Citations

- **QLoRA**:\
  Dettmers, T., Pagnoni, A., Holtzman, A., & Zettlemoyer, L. (2023). QLoRA: Efficient Finetuning of Quantized LLMs. *arXiv preprint arXiv:2305.14314*. https://arxiv.org/abs/2305.14314
- **Qwen**:\
  Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., ... Zhu, S. (2023). Qwen Technical Report. *arXiv preprint arXiv:2309.16609*. https://arxiv.org/abs/2309.16609

## Acknowledgments

- The fine-tuning process was conducted using Google Colab's T4 GPU.
- The dataset was sourced from https://huggingface.co/datasets/Gryphe/ChatGPT-4o-Writing-Prompts
- Special thanks to the developers of Qwen and QLoRA for their foundational work.
